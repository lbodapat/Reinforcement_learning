{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym in /Users/surajbodapati/opt/anaconda3/lib/python3.8/site-packages (0.19.0)\r\n",
      "Requirement already satisfied: numpy>=1.18.0 in /Users/surajbodapati/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.20.1)\r\n",
      "Requirement already satisfied: cloudpickle<1.7.0,>=1.2.0 in /Users/surajbodapati/opt/anaconda3/lib/python3.8/site-packages (from gym) (1.6.0)\r\n"
     ]
    }
   ],
   "source": [
    "! pip install gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Project Description:\n",
    "\n",
    "Deterministic environmet:\n",
    "Actions: 4 - left, right, top, bottom\n",
    "States: 16 states (4*4 grid)\n",
    "Rewards: \n",
    "Three positive rewards- Drinking Coffee-0.3, Studying-0.6, End Goal-1\n",
    "Two Negative rewards- Bunk class- -0.5\n",
    "\n",
    "Deterministic environmet:\n",
    "Actions: 5 - left, right, top, bottom, stay in same position\n",
    "States: 16 states (4*4 grid)\n",
    "Rewards: \n",
    "Three positive rewards- Drinking Coffee-0.3, Studying-0.6, End Goal-1\n",
    "Two Negative rewards- Bunk class- -0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "import matplotlib.pyplot as plt\n",
    "from time import sleep\n",
    "from scipy.stats import binom\n",
    "from operator import add\n",
    "from matplotlib.offsetbox import (TextArea, DrawingArea, OffsetImage,\n",
    "                                  AnnotationBbox)\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GridEnvironment(gym.Env):\n",
    "    \n",
    "    def __init__(self):\n",
    "        print(\"Intitializing the Env....\")\n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [3, 3]\n",
    "        self.positive_reward_1=[2,2]\n",
    "        self.positive_reward_2=[1,1]\n",
    "        self.negative_reward_1=[2,0]\n",
    "        self.negative_reward_2=[0,2]\n",
    "        \n",
    "        self.state = np.zeros((4, 4))\n",
    "        observation = self.state.flatten()\n",
    "        self.observation_space = spaces.Discrete(16)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 100     \n",
    "        self.qtable=np.zeros((self.observation_space.n,self.action_space.n))\n",
    "        \n",
    "        values=[i for i in range(16)]\n",
    "        keys=[]\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                keys.append((i,j))\n",
    "        self.mapping=dict(zip(keys,values))\n",
    "        \n",
    "        self.num_of_episodes=200\n",
    "        self.reward_per_episode_array=[]\n",
    "        self.qtables_array=[]\n",
    "        self.rewards_in_episode_for_optimal_policy=[]\n",
    "        self.winning_state=[]\n",
    "        self.episilon_decay_array=[]\n",
    "        \n",
    "    def reset(self):\n",
    "        self.timestep = 0\n",
    "        self.agent_pos = [0, 0]\n",
    "        self.goal_pos = [3, 3]\n",
    "        self.positive_reward_1=[2,2]\n",
    "        self.positive_reward_2=[1,1]\n",
    "        self.negative_reward_1=[2,0]\n",
    "        self.negative_reward_2=[0,2]\n",
    "        self.state = np.zeros((4, 4)) \n",
    "#         self.qtable=np.zeros((self.observation_space.n,self.action_space.n))\n",
    "        observation = self.state.flatten()\n",
    "        self.observation_space = spaces.Discrete(16)\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.max_timesteps = 100\n",
    "        values=[i for i in range(16)]\n",
    "        keys=[]\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                keys.append((i,j))\n",
    "        self.mapping=dict(zip(keys,values))\n",
    "        \n",
    "        return observation\n",
    "    \n",
    "    def step(self,env_type,action):\n",
    "        if(env_type=='D'):\n",
    "            if action == 0:\n",
    "                self.agent_pos[0] += 1\n",
    "            if action == 1:\n",
    "                self.agent_pos[0] -= 1\n",
    "            if action == 2:\n",
    "                self.agent_pos[1] += 1\n",
    "            if action == 3:\n",
    "                self.agent_pos[1] -= 1\n",
    "        else:\n",
    "            #random value here is either or 1\n",
    "            random_value=self.get_random_value_by_probability()\n",
    "            if action == 0:\n",
    "                self.agent_pos[0] += random_value\n",
    "            if action == 1:\n",
    "                self.agent_pos[0] -= random_value\n",
    "            if action == 2:\n",
    "                self.agent_pos[1] += random_value\n",
    "            if action == 3:\n",
    "                self.agent_pos[1] -= random_value\n",
    "                \n",
    "        self.agent_pos = np.clip(self.agent_pos, 0, 3)\n",
    "        \n",
    "        info = {}        \n",
    "        observation = self.state.flatten()\n",
    "        reward = 0\n",
    "        if (self.agent_pos == self.goal_pos).all():\n",
    "            reward = 10\n",
    "            done=True\n",
    "            new_agent_pos=self.mapping[tuple(self.agent_pos)]\n",
    "            return new_agent_pos, reward, done, info\n",
    "        elif (self.agent_pos == self.positive_reward_1).all():\n",
    "            reward=6\n",
    "        elif (self.agent_pos == self.positive_reward_2).all():\n",
    "            reward=3\n",
    "        elif(self.agent_pos == self.negative_reward_1).all():\n",
    "            reward= -5\n",
    "        elif(self.agent_pos == self.negative_reward_2).all():\n",
    "            reward= -6\n",
    "            \n",
    "        self.timestep += 1\n",
    "        done = True if self.timestep >= self.max_timesteps else False\n",
    "        \n",
    "        new_agent_pos=self.mapping[tuple(self.agent_pos)]\n",
    "        return new_agent_pos, reward, done, info\n",
    "    \n",
    "    def get_random_value_by_probability(self):\n",
    "        x=np.random.randint(0, 10)\n",
    "        random_value=0\n",
    "        if x <= 6:\n",
    "            random_value=1\n",
    "        print(\"RANDOM VAL\",random_value)\n",
    "        return random_value\n",
    "    \n",
    "    def render(self):\n",
    "        plt.imshow(self.state)\n",
    "        \n",
    "    def render2(self,mode='human',plot=False):\n",
    "        fig,ax=plt.subplots(2,2,figsize=(10,10))\n",
    "        ax.set_xlim(0,4)\n",
    "        ax.set_ylim(0,4)\n",
    "                \n",
    "        agent = AnnotationBbox(OffsetImage(plt.imread('./images/robot/Robot_pos1.png'), zoom=0.4),  # Plotting the agent.\n",
    "                           list(map(add, self.agent_pos, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(agent)\n",
    "\n",
    "        if (self.agent_pos == self.goal_pos).all():\n",
    "            goal = AnnotationBbox(OffsetImage(plt.imread('./images/robot1/grad.png'), zoom=0.12),  # Plotting the agent.\n",
    "                           list(map(add, self.goal_pos, [0.5, 0.5])), frameon=False)\n",
    "            agent.remove()\n",
    "        else:\n",
    "            goal = AnnotationBbox(OffsetImage(plt.imread('./images/RL-696x309.png'), zoom=0.2),  # Plotting the agent.\n",
    "                           list(map(add, self.goal_pos, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(goal)\n",
    "        \n",
    "        if(self.agent_pos == self.negative_reward_1).all():\n",
    "            negative_rw_1 = AnnotationBbox(OffsetImage(plt.imread('./images/robot1/Conf_bunk.png'), zoom=0.11),  # Plotting the agent.\n",
    "                           list(map(add, self.negative_reward_1, [0.5, 0.5])), frameon=False)\n",
    "            agent.remove()\n",
    "        else:\n",
    "            negative_rw_1 = AnnotationBbox(OffsetImage(plt.imread('./images/bunk.jpeg'), zoom=0.4),  # Plotting the agent.\n",
    "                           list(map(add, self.negative_reward_1, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(negative_rw_1)\n",
    "        \n",
    "        if(self.agent_pos == self.negative_reward_2).all():\n",
    "            negative_rw_2 = AnnotationBbox(OffsetImage(plt.imread('./images/robot1/Conf_bunk.png'), zoom=0.11),  # Plotting the agent.\n",
    "                           list(map(add, self.negative_reward_2, [0.5, 0.5])), frameon=False)\n",
    "            agent.remove()\n",
    "        else:\n",
    "            negative_rw_2 = AnnotationBbox(OffsetImage(plt.imread('./images/bunk.jpeg'), zoom=0.4),  # Plotting the agent.\n",
    "                           list(map(add, self.negative_reward_2, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(negative_rw_2)\n",
    "        \n",
    "        if(self.agent_pos == self.positive_reward_1).all():\n",
    "            positive_rw_1 = AnnotationBbox(OffsetImage(plt.imread('./images/robot1/assignment.png'), zoom=0.21),  # Plotting the agent.\n",
    "                           list(map(add, self.positive_reward_1, [0.5, 0.5])), frameon=False)\n",
    "            agent.remove()\n",
    "        else:\n",
    "            positive_rw_1 = AnnotationBbox(OffsetImage(plt.imread('./images/robot1/books.png'), zoom=0.15),  # Plotting the agent.\n",
    "                           list(map(add, self.positive_reward_1, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(positive_rw_1)\n",
    "        \n",
    "        if(self.agent_pos == self.positive_reward_2).all():\n",
    "            positive_rw_2 = AnnotationBbox(OffsetImage(plt.imread('./images/robot1/robot_coffee_final.png'), zoom=0.25),  # Plotting the agent.\n",
    "                           list(map(add, self.positive_reward_2, [0.5, 0.5])), frameon=False)\n",
    "            agent.remove()\n",
    "        else:\n",
    "            positive_rw_2 = AnnotationBbox(OffsetImage(plt.imread('./images/coffee.png'), zoom=0.15),  # Plotting the agent.\n",
    "                           list(map(add, self.positive_reward_2, [0.5, 0.5])), frameon=False)\n",
    "        ax.add_artist(positive_rw_2)\n",
    "        \n",
    "        ax.set_xticks(np.arange(0, 4, 1))\n",
    "        ax.set_yticks(np.arange(0, 4, 1))\n",
    "        plt.grid(fig, linewidth=1,which='both', axis='both')\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndTestModels():\n",
    "    def __init__(self):\n",
    "        print(\"Training and Testing the Models\")\n",
    "        \n",
    "    def navigate(self,function_type,env_type):\n",
    "        env = GridEnvironment()\n",
    "        if(function_type=='QL'):\n",
    "            qtable_Q_learning=self.getQTableQL(env_type,env)\n",
    "        elif(function_type=='SARSA'):\n",
    "            qtable_Q_learning=self.getQTableSARSA(env_type,env)\n",
    "\n",
    "    def getQTableQL(self,env_type,env):\n",
    "        print(\"<-------------------Q-Learning-------------------->\")\n",
    "        if(env_type=='D'):\n",
    "            print(\"<-------------------Deterministic Environment-------------------->\")\n",
    "        elif(env_type=='S'):\n",
    "            print(\"<-------------------Stochastic Environment-------------------->\")\n",
    "        for episode in range(env.num_of_episodes):\n",
    "            #learning rate\n",
    "            alpha=0.4\n",
    "            #discount rate\n",
    "            gamma=0.7\n",
    "    \n",
    "            exploration_rate = 1\n",
    "            max_exploration_rate = 1\n",
    "            min_exploration_rate = 0.01\n",
    "            exploration_decay_rate = 0.01\n",
    "        \n",
    "            env.reset()\n",
    "            cum_reward_per_episode=0\n",
    "            actions_in_one_episode=[]\n",
    "            states_passed_in_one_episode=[]\n",
    "            rewards_in_episode=[]\n",
    "            states_passed_in_one_episode.append(env.mapping[tuple(env.agent_pos)])\n",
    "            greedy_a=0\n",
    "            random_a=1\n",
    "            while True:\n",
    "                state=env.mapping[tuple(env.agent_pos)]\n",
    "                qtable_state=env.qtable[state]\n",
    "                greedy_action=np.argmax(qtable_state)\n",
    "                greedy_q_value= max(qtable_state)\n",
    "        \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    action=greedy_action\n",
    "                    greedy_a=greedy_a+1\n",
    "                else:\n",
    "                    action=env.action_space.sample()\n",
    "                    random_a=random_a+1\n",
    "                env.episilon_decay_array.append(exploration_rate)\n",
    "\n",
    "                new_observation, reward, done, _ = env.step('D',action)\n",
    "                        \n",
    "                if(new_observation in states_passed_in_one_episode):\n",
    "                    reward= -1            \n",
    "                if(new_observation==state):\n",
    "                    reward= -10\n",
    "            \n",
    "                qtable_state[action]=qtable_state[action]+(alpha*(reward+gamma*max(env.qtable[new_observation])-qtable_state[[action]]))\n",
    "            \n",
    "                env.qtable[state]=qtable_state\n",
    "            \n",
    "                actions_in_one_episode.append(action)\n",
    "                states_passed_in_one_episode.append(new_observation)\n",
    "                cum_reward_per_episode=cum_reward_per_episode+reward\n",
    "                rewards_in_episode.append(reward)\n",
    "\n",
    "                exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "            \n",
    "                if done==True:\n",
    "                    print(\"One episode complete\")\n",
    "                    print(\"Reward for this episode is: \",cum_reward_per_episode)\n",
    "                    print(\"Actions taken during this episode: \",actions_in_one_episode)\n",
    "                    print(\"States traversed during this episode: \",states_passed_in_one_episode)\n",
    "                    print(\"Greedy actions taken\",greedy_a,\"Random actions taken\",random_a)\n",
    "                    env.reward_per_episode_array.append(cum_reward_per_episode)\n",
    "                    env.qtables_array.append(env.qtable)\n",
    "                    if(cum_reward_per_episode >= max(env.reward_per_episode_array)):\n",
    "                        env.winning_state=states_passed_in_one_episode\n",
    "                        env.rewards_in_episode_for_optimal_policy=rewards_in_episode\n",
    "                    break\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Maximum reward in all episodes: \",max(env.reward_per_episode_array))\n",
    "        print(\"Winning State Actions: \",env.winning_state)\n",
    "        print(\"Final Q Table\",env.qtable)\n",
    "        plt.plot(env.reward_per_episode_array)\n",
    "        plt.figure() \n",
    "        plt.plot(env.episilon_decay_array)\n",
    "#         self.display(env.winning_state,env.rewards_in_episode_for_optimal_policy)\n",
    "        self.display2(env.qtable)\n",
    "        return env.qtable\n",
    "\n",
    "\n",
    "    def getQTableSARSA(self,env_type,env):\n",
    "        print(\"<-------------------SARSA-------------------->\")\n",
    "        if(env_type=='D'):\n",
    "            print(\"<-------------------Deterministic Environment-------------------->\")\n",
    "        elif(env_type=='S'):\n",
    "            print(\"<-------------------Stochastic Environment-------------------->\")\n",
    "        \n",
    "        for episode in range(env.num_of_episodes):\n",
    "            #learning rate\n",
    "            alpha=0.4\n",
    "            #discount rate\n",
    "            gamma=0.7\n",
    "    \n",
    "            exploration_rate = 1\n",
    "            max_exploration_rate = 1\n",
    "            min_exploration_rate = 0.01\n",
    "            exploration_decay_rate = 0.01\n",
    "        \n",
    "            env.reset()\n",
    "            cum_reward_per_episode=0\n",
    "            actions_in_one_episode=[]\n",
    "            states_passed_in_one_episode=[]\n",
    "            rewards_in_episode=[]\n",
    "            states_passed_in_one_episode.append(env.mapping[tuple(env.agent_pos)])\n",
    "            greedy_a=0\n",
    "            random_a=1\n",
    "            while True:\n",
    "                state=env.mapping[tuple(env.agent_pos)]\n",
    "                qtable_state=env.qtable[state]\n",
    "                greedy_action=np.argmax(qtable_state)\n",
    "                greedy_q_value= max(qtable_state)\n",
    "        \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    action=greedy_action\n",
    "                    greedy_a=greedy_a+1\n",
    "                else:\n",
    "                    action=env.action_space.sample()\n",
    "                    random_a=random_a+1\n",
    "                \n",
    "                env.episilon_decay_array.append(exploration_rate)\n",
    "            \n",
    "                new_observation, reward, done, _ = env.step('D',action)\n",
    "                        \n",
    "                if(new_observation in states_passed_in_one_episode):\n",
    "                    reward= -1            \n",
    "                if(new_observation==state):\n",
    "                    reward= -10\n",
    "           \n",
    "                exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "           \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    new_action=greedy_action\n",
    "                else:\n",
    "                    new_action=env.action_space.sample()\n",
    "                \n",
    "                qtable_state[action]=qtable_state[action]+(alpha*(reward+gamma*(env.qtable[new_observation][new_action])-qtable_state[[action]]))\n",
    "            \n",
    "                env.qtable[state]=qtable_state\n",
    "            \n",
    "                actions_in_one_episode.append(action)\n",
    "                states_passed_in_one_episode.append(new_observation)\n",
    "                cum_reward_per_episode=cum_reward_per_episode+reward\n",
    "                rewards_in_episode.append(reward)\n",
    "                        \n",
    "                if done==True:\n",
    "                    print(\"One episode complete\")\n",
    "                    print(\"Reward for this episode is: \",cum_reward_per_episode)\n",
    "                    print(\"Actions taken during this episode: \",actions_in_one_episode)\n",
    "                    print(\"States traversed during this episode: \",states_passed_in_one_episode)\n",
    "                    print(\"Greedy actions taken\",greedy_a,\"Random actions taken\",random_a)\n",
    "                    env.reward_per_episode_array.append(cum_reward_per_episode)\n",
    "                    env.qtables_array.append(env.qtable)\n",
    "                    if(cum_reward_per_episode >= max(env.reward_per_episode_array)):\n",
    "                        env.winning_state=states_passed_in_one_episode\n",
    "                        env.rewards_in_episode_for_optimal_policy=rewards_in_episode\n",
    "                    break\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Maximum reward in all episodes: \",max(env.reward_per_episode_array))\n",
    "        print(\"Winning State Traversal: \",env.winning_state)\n",
    "        print(\"FINAL Q TABLE: \",env.qtable)\n",
    "        plt.plot(env.reward_per_episode_array)\n",
    "        plt.figure() \n",
    "        plt.plot(env.episilon_decay_array)\n",
    "#     display(env.winning_state,env.rewards_in_episode_for_optimal_policy)\n",
    "        self.display2(env.qtable)\n",
    "        return env.qtable\n",
    "\n",
    "    def display(self,winning_states,winning_state_rewards):\n",
    "        print(\"Winning state rewards\",winning_state_rewards)\n",
    "        inv_map = {v: k for k, v in env.mapping.items()}\n",
    "        for i in range(len(winning_states)):\n",
    "            print(list(inv_map.get(winning_states[i])))\n",
    "            env.agent_pos=list(inv_map.get(winning_states[i]))\n",
    "            env.render2()\n",
    "            sleep(.1)\n",
    "\n",
    "    def display2(self,qtable):\n",
    "        env.reset()\n",
    "        print(\"Using Q table to move though the optimal states......\")\n",
    "        state=env.mapping[(0,0)]\n",
    "        inv_map = {v: k for k, v in env.mapping.items()}\n",
    "        while True:\n",
    "            qtable_state=env.qtable[state]\n",
    "            optimal_action=np.argmax(qtable_state)\n",
    "            new_observation, reward, done, _ = env.step('D',optimal_action)\n",
    "            state=new_observation\n",
    "            print(list(inv_map.get(state)))\n",
    "            env.agent_pos=list(inv_map.get(state))\n",
    "            env.render2()\n",
    "            sleep(.1)\n",
    "            if(done):\n",
    "                break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and Testing the Models\n",
      "Intitializing the Env....\n",
      "<-------------------Q-Learning-------------------->\n",
      "<-------------------Deterministic Environment-------------------->\n",
      "One episode complete\n",
      "Reward for this episode is:  -33\n",
      "Actions taken during this episode:  [0, 1, 3, 1, 0, 2, 0, 3, 2, 3, 3, 1, 2, 2, 2, 3, 2, 1, 3, 0, 3, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 0, 0, 0, 4, 5, 9, 8, 9, 8, 8, 4, 5, 6, 7, 6, 7, 3, 2, 6, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 0 Random actions taken 26\n",
      "One episode complete\n",
      "Reward for this episode is:  -61\n",
      "Actions taken during this episode:  [1, 0, 3, 1, 1, 0, 1, 0, 2, 3, 3, 3, 1, 3, 0, 3, 2, 2, 3, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 4, 0, 0, 4, 0, 4, 5, 4, 4, 4, 0, 0, 4, 4, 5, 6, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 1 Random actions taken 23\n",
      "One episode complete\n",
      "Reward for this episode is:  -269\n",
      "Actions taken during this episode:  [0, 3, 3, 1, 2, 0, 0, 0, 1, 0, 0, 1, 2, 0, 3, 0, 3, 3, 1, 3, 1, 1, 3, 0, 2, 1, 0, 1, 0, 2, 3, 2, 1, 2, 0, 2, 1, 2, 3, 2, 0, 3, 1, 1, 1, 2, 1, 3, 2, 3, 3, 0, 3, 2, 2, 1, 2, 1, 0, 1, 2, 0, 2, 1, 2, 0, 1, 3, 2, 0, 3, 1, 3, 2, 2, 2, 3, 2, 3, 0, 1, 0, 3, 2, 1, 0, 3, 0, 1, 1, 2, 0, 1, 3, 0, 3, 1, 1, 1, 3]\n",
      "States traversed during this episode:  [0, 4, 4, 4, 0, 1, 5, 9, 13, 9, 13, 13, 9, 10, 14, 13, 13, 12, 12, 8, 8, 4, 0, 0, 4, 5, 1, 5, 1, 5, 6, 5, 6, 2, 3, 7, 7, 3, 3, 2, 3, 7, 6, 2, 2, 2, 3, 3, 2, 3, 2, 1, 5, 4, 5, 6, 2, 3, 3, 7, 3, 3, 7, 7, 3, 3, 7, 3, 2, 3, 7, 6, 2, 1, 2, 3, 3, 2, 3, 2, 6, 2, 6, 5, 6, 2, 6, 5, 9, 5, 1, 2, 6, 2, 1, 5, 4, 0, 0, 0, 0]\n",
      "Greedy actions taken 4 Random actions taken 97\n",
      "One episode complete\n",
      "Reward for this episode is:  -218\n",
      "Actions taken during this episode:  [3, 1, 2, 3, 3, 1, 3, 3, 0, 1, 3, 0, 3, 2, 2, 3, 3, 0, 2, 1, 3, 2, 1, 2, 3, 2, 1, 1, 3, 0, 1, 3, 2, 1, 1, 3, 3, 2, 3, 0, 0, 1, 2, 3, 1, 3, 1, 2, 1, 2, 3, 2, 0, 0, 0, 3, 3, 1, 0, 1, 0, 3, 3, 0, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 0, 0, 0, 0, 0, 4, 0, 0, 4, 4, 5, 6, 5, 4, 8, 9, 5, 4, 5, 1, 2, 1, 2, 2, 2, 1, 5, 1, 0, 1, 1, 1, 0, 0, 1, 0, 4, 8, 4, 5, 4, 0, 0, 0, 1, 1, 2, 1, 2, 6, 10, 14, 13, 12, 8, 12, 8, 12, 12, 12, 12, 13, 14, 15]\n",
      "Greedy actions taken 0 Random actions taken 68\n",
      "One episode complete\n",
      "Reward for this episode is:  -207\n",
      "Actions taken during this episode:  [2, 0, 2, 3, 1, 2, 2, 2, 2, 0, 0, 1, 3, 2, 3, 1, 2, 2, 3, 0, 3, 1, 3, 2, 0, 0, 2, 0, 3, 0, 3, 2, 0, 2, 3, 3, 3, 3, 0, 3, 3, 2, 1, 1, 3, 1, 0, 0, 2, 3, 3, 1, 2, 3, 2, 1, 3, 2, 0, 3, 1, 1, 0, 3, 1, 2, 3, 0, 2, 1, 0, 3, 1, 0, 2, 0, 0, 0, 3, 3, 1, 3, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 5, 1, 2, 3, 3, 3, 7, 11, 7, 6, 7, 6, 2, 3, 3, 2, 6, 5, 1, 0, 1, 5, 9, 10, 14, 13, 13, 12, 13, 13, 14, 13, 12, 12, 12, 12, 12, 12, 13, 9, 5, 4, 0, 4, 8, 9, 8, 8, 4, 5, 4, 5, 1, 0, 1, 5, 4, 0, 0, 4, 4, 0, 1, 0, 4, 5, 1, 5, 4, 0, 4, 5, 9, 13, 13, 12, 12, 8, 8, 9, 10, 11, 15]\n",
      "Greedy actions taken 3 Random actions taken 84\n",
      "One episode complete\n",
      "Reward for this episode is:  -146\n",
      "Actions taken during this episode:  [3, 2, 3, 3, 3, 0, 0, 2, 3, 3, 0, 1, 0, 0, 1, 3, 3, 2, 0, 2, 1, 3, 0, 1, 3, 0, 1, 3, 2, 3, 1, 0, 3, 2, 3, 3, 1, 1, 3, 3, 2, 1, 2, 0, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 0, 0, 0, 4, 8, 9, 8, 8, 12, 8, 12, 12, 8, 8, 8, 9, 13, 14, 10, 9, 13, 9, 8, 12, 8, 8, 9, 8, 4, 8, 8, 9, 8, 8, 4, 0, 0, 0, 1, 1, 2, 6, 7, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 46\n",
      "One episode complete\n",
      "Reward for this episode is:  -337\n",
      "Actions taken during this episode:  [1, 0, 3, 1, 2, 0, 2, 1, 2, 0, 2, 2, 3, 2, 0, 2, 3, 3, 2, 1, 1, 0, 1, 1, 2, 1, 0, 1, 1, 0, 3, 0, 0, 0, 3, 2, 0, 1, 3, 2, 1, 3, 0, 2, 3, 0, 2, 1, 2, 3, 1, 3, 3, 1, 2, 2, 0, 3, 2, 2, 1, 1, 3, 2, 2, 3, 2, 1, 3, 1, 2, 2, 2, 1, 2, 2, 1, 1, 1, 3, 3, 2, 2, 0, 1, 2, 2, 3, 1, 0, 2, 0, 1, 3, 2, 2, 1, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 4, 0, 1, 5, 6, 2, 3, 7, 7, 7, 6, 7, 11, 11, 10, 9, 10, 6, 2, 6, 2, 2, 3, 3, 7, 3, 3, 7, 6, 10, 14, 14, 13, 14, 14, 10, 9, 10, 6, 5, 9, 10, 9, 13, 14, 10, 11, 10, 6, 5, 4, 0, 1, 2, 6, 5, 6, 7, 3, 3, 2, 3, 3, 2, 3, 3, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 2, 1, 2, 3, 7, 3, 3, 3, 2, 2, 6, 7, 11, 7, 6, 7, 7, 3, 3, 7, 7]\n",
      "Greedy actions taken 6 Random actions taken 95\n",
      "One episode complete\n",
      "Reward for this episode is:  -73\n",
      "Actions taken during this episode:  [1, 3, 2, 2, 1, 2, 0, 0, 2, 1, 0, 1, 1, 3, 1, 0, 2, 2, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 2, 2, 3, 7, 11, 11, 7, 11, 7, 3, 2, 2, 6, 7, 7, 7, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 20\n",
      "One episode complete\n",
      "Reward for this episode is:  -103\n",
      "Actions taken during this episode:  [3, 2, 3, 2, 0, 3, 0, 3, 3, 3, 2, 2, 2, 1, 0, 2, 1, 3, 0, 1, 2, 3, 3, 2, 0, 1, 3, 0, 0, 1, 1, 3, 2, 0, 0, 2, 3, 1, 1, 0, 3, 2, 2, 3, 2, 0, 3, 0, 0, 3, 1, 3, 1, 0, 2, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 0, 1, 5, 4, 8, 8, 8, 8, 9, 10, 11, 7, 11, 11, 7, 6, 10, 6, 7, 6, 5, 6, 10, 6, 5, 9, 13, 9, 5, 4, 5, 9, 13, 14, 13, 9, 5, 9, 8, 9, 10, 9, 10, 14, 13, 13, 13, 12, 8, 8, 4, 8, 9, 13, 14, 15]\n",
      "Greedy actions taken 3 Random actions taken 56\n",
      "One episode complete\n",
      "Reward for this episode is:  -38\n",
      "Actions taken during this episode:  [2, 2, 0, 1, 1, 3, 1, 1, 2, 0, 0, 0, 1, 2, 1, 0, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 2, 6, 2, 2, 1, 1, 1, 2, 6, 10, 14, 10, 11, 7, 11, 11, 7, 11, 15]\n",
      "Greedy actions taken 1 Random actions taken 20\n",
      "One episode complete\n",
      "Reward for this episode is:  -10\n",
      "Actions taken during this episode:  [2, 0, 1, 2, 0, 0, 3, 2, 0, 3, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 1, 2, 6, 10, 9, 10, 14, 13, 14, 14, 14, 15]\n",
      "Greedy actions taken 2 Random actions taken 13\n",
      "One episode complete\n",
      "Reward for this episode is:  -19\n",
      "Actions taken during this episode:  [3, 3, 2, 3, 3, 2, 0, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 0, 0, 1, 5, 9, 13, 14, 15]\n",
      "Greedy actions taken 2 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  -156\n",
      "Actions taken during this episode:  [2, 3, 2, 3, 1, 0, 2, 0, 2, 1, 1, 0, 1, 2, 3, 1, 1, 0, 1, 1, 1, 0, 1, 3, 3, 3, 2, 3, 2, 2, 3, 2, 3, 2, 3, 2, 2, 2, 2, 3, 1, 3, 2, 2, 0, 3, 2, 3, 1, 0, 1, 0, 1, 1, 0, 3, 0, 1, 2, 2, 1, 3, 0, 3, 0, 3, 0, 3, 1, 2, 3, 2, 1, 0, 0, 3, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 0, 1, 0, 0, 4, 5, 9, 10, 6, 2, 6, 2, 3, 2, 2, 2, 6, 2, 2, 2, 6, 2, 1, 0, 0, 1, 0, 1, 2, 1, 2, 1, 2, 1, 2, 3, 3, 3, 2, 2, 1, 2, 3, 7, 6, 7, 6, 2, 6, 2, 6, 2, 2, 6, 5, 9, 5, 6, 7, 3, 2, 6, 5, 9, 8, 12, 12, 8, 9, 8, 9, 5, 9, 13, 12, 13, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 76\n",
      "One episode complete\n",
      "Reward for this episode is:  -39\n",
      "Actions taken during this episode:  [3, 3, 1, 0, 1, 0, 1, 2, 0, 0, 1, 0, 3, 2, 3, 0, 0, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 0, 0, 4, 0, 4, 0, 1, 5, 9, 5, 9, 8, 9, 8, 12, 12, 13, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 17\n",
      "One episode complete\n",
      "Reward for this episode is:  -59\n",
      "Actions taken during this episode:  [2, 1, 3, 3, 0, 2, 2, 1, 2, 1, 2, 2, 3, 3, 0, 2, 0, 2, 2, 3, 1, 1, 2, 3, 0, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 1, 0, 0, 4, 5, 6, 2, 3, 3, 3, 3, 2, 1, 5, 6, 10, 11, 11, 10, 6, 2, 3, 2, 6, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 25\n",
      "One episode complete\n",
      "Reward for this episode is:  -51\n",
      "Actions taken during this episode:  [2, 2, 1, 1, 1, 0, 2, 2, 3, 0, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 2, 2, 2, 2, 6, 7, 7, 6, 10, 14, 14, 14, 15]\n",
      "Greedy actions taken 1 Random actions taken 14\n",
      "One episode complete\n",
      "Reward for this episode is:  -6\n",
      "Actions taken during this episode:  [2, 2, 1, 2, 0, 3, 3, 2, 3, 2, 0, 0, 3, 2, 3, 2, 1, 3, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 2, 2, 3, 7, 6, 5, 6, 5, 6, 10, 14, 13, 14, 13, 14, 10, 9, 13, 14, 15]\n",
      "Greedy actions taken 1 Random actions taken 21\n",
      "One episode complete\n",
      "Reward for this episode is:  -142\n",
      "Actions taken during this episode:  [3, 1, 0, 2, 0, 1, 3, 2, 2, 2, 1, 0, 2, 1, 0, 1, 1, 3, 2, 2, 0, 0, 2, 1, 2, 2, 1, 0, 2, 1, 0, 3, 3, 2, 0, 1, 1, 2, 0, 2, 1, 0, 2, 0, 3, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 0, 4, 5, 9, 5, 4, 5, 6, 7, 3, 7, 7, 3, 7, 3, 3, 2, 3, 3, 7, 11, 11, 7, 7, 7, 3, 7, 7, 3, 7, 6, 5, 6, 10, 6, 2, 3, 7, 7, 3, 7, 7, 11, 10, 14, 14, 14, 15]\n",
      "Greedy actions taken 10 Random actions taken 40\n",
      "One episode complete\n",
      "Reward for this episode is:  -2\n",
      "Actions taken during this episode:  [3, 2, 3, 0, 2, 0, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 0, 4, 5, 9, 10, 14, 14, 15]\n",
      "Greedy actions taken 2 Random actions taken 9\n",
      "One episode complete\n",
      "Reward for this episode is:  -5\n",
      "Actions taken during this episode:  [2, 1, 3, 0, 3, 2, 0, 1, 2, 0, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 1, 0, 4, 4, 5, 9, 5, 6, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 13\n",
      "One episode complete\n",
      "Reward for this episode is:  1\n",
      "Actions taken during this episode:  [0, 0, 2, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 8, 9, 10, 11, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  -91\n",
      "Actions taken during this episode:  [2, 2, 0, 0, 0, 0, 3, 3, 1, 3, 0, 3, 3, 1, 0, 3, 0, 2, 0, 1, 2, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 2, 6, 10, 14, 14, 13, 12, 8, 8, 12, 12, 12, 8, 12, 12, 12, 13, 13, 9, 10, 14, 14, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 21\n",
      "One episode complete\n",
      "Reward for this episode is:  -38\n",
      "Actions taken during this episode:  [0, 0, 3, 0, 2, 0, 3, 0, 1, 2, 0, 1, 2, 0, 1, 1, 0, 1, 2, 0, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 4, 8, 8, 12, 13, 13, 12, 12, 8, 9, 13, 9, 10, 14, 10, 6, 10, 6, 7, 11, 11, 7, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 23\n",
      "One episode complete\n",
      "Reward for this episode is:  -49\n",
      "Actions taken during this episode:  [2, 0, 2, 1, 0, 3, 0, 2, 1, 0, 3, 2, 3, 3, 2, 0, 3, 1, 3, 2, 3, 3, 2, 1, 1, 0, 0, 2, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 2, 6, 5, 9, 10, 6, 10, 9, 10, 9, 8, 9, 13, 12, 8, 8, 9, 8, 8, 9, 5, 1, 5, 9, 10, 14, 14, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 28\n",
      "One episode complete\n",
      "Reward for this episode is:  18\n",
      "Actions taken during this episode:  [2, 3, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  10\n",
      "Actions taken during this episode:  [0, 0, 2, 2, 1, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 8, 9, 10, 6, 10, 14, 15]\n",
      "Greedy actions taken 1 Random actions taken 8\n",
      "One episode complete\n",
      "Reward for this episode is:  -98\n",
      "Actions taken during this episode:  [0, 0, 1, 0, 3, 2, 1, 2, 1, 2, 0, 1, 1, 3, 3, 3, 1, 0, 0, 2, 3, 1, 1, 3, 3, 0, 3, 0, 1, 2, 1, 0, 2, 3, 2, 1, 0, 3, 0, 3, 2, 2, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 8, 4, 8, 8, 9, 5, 6, 2, 3, 7, 3, 3, 2, 1, 0, 0, 4, 8, 9, 8, 4, 0, 0, 0, 4, 4, 8, 4, 5, 1, 5, 6, 5, 6, 2, 6, 5, 9, 8, 9, 10, 14, 14, 14, 15]\n",
      "Greedy actions taken 8 Random actions taken 39\n",
      "One episode complete\n",
      "Reward for this episode is:  6\n",
      "Actions taken during this episode:  [0, 2, 1, 0, 3, 0, 2, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 1, 5, 4, 8, 9, 13, 14, 15]\n",
      "Greedy actions taken 3 Random actions taken 8\n",
      "One episode complete\n",
      "Reward for this episode is:  -102\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 2, 1, 3, 0, 0, 0, 3, 1, 2, 1, 0, 0, 1, 3, 3, 3, 2, 2, 2, 3, 0, 1, 0, 0, 0, 1, 3, 2, 3, 3, 3, 2, 0, 1, 2, 0, 1, 2, 1, 0, 2, 2, 3, 1, 3, 3, 2, 0, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 11, 7, 6, 10, 14, 14, 13, 9, 10, 6, 10, 14, 10, 9, 8, 8, 9, 10, 11, 10, 14, 10, 14, 14, 14, 10, 9, 10, 9, 8, 8, 9, 13, 9, 10, 14, 10, 11, 7, 11, 11, 11, 10, 6, 5, 4, 5, 9, 13, 13, 14, 15]\n",
      "Greedy actions taken 10 Random actions taken 47\n",
      "One episode complete\n",
      "Reward for this episode is:  -3\n",
      "Actions taken during this episode:  [1, 0, 2, 1, 3, 0, 2, 3, 0, 2, 2, 3, 2, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 1, 0, 4, 5, 4, 8, 9, 10, 9, 10, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 14\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [2, 3, 0, 2, 2, 3, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 0, 4, 5, 6, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 2 Random actions taken 9\n",
      "One episode complete\n",
      "Reward for this episode is:  -26\n",
      "Actions taken during this episode:  [3, 0, 0, 2, 3, 2, 3, 2, 3, 3, 3, 0, 2, 2, 1, 2, 3, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 8, 9, 8, 9, 8, 9, 8, 8, 8, 12, 13, 14, 10, 11, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 14\n",
      "One episode complete\n",
      "Reward for this episode is:  -2\n",
      "Actions taken during this episode:  [0, 0, 1, 2, 0, 2, 2, 3, 3, 1, 1, 0, 0, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 8, 4, 5, 9, 10, 11, 10, 9, 5, 1, 5, 9, 13, 13, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 13\n",
      "One episode complete\n",
      "Reward for this episode is:  -4\n",
      "Actions taken during this episode:  [1, 0, 2, 0, 2, 3, 2, 1, 3, 1, 0, 0, 3, 2, 2, 0, 3, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 9, 10, 9, 10, 6, 5, 1, 5, 9, 8, 9, 10, 14, 13, 14, 15]\n",
      "Greedy actions taken 8 Random actions taken 12\n",
      "One episode complete\n",
      "Reward for this episode is:  1\n",
      "Actions taken during this episode:  [1, 0, 0, 2, 1, 0, 2, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 8, 9, 5, 9, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  -7\n",
      "Actions taken during this episode:  [2, 0, 3, 2, 3, 1, 0, 0, 2, 3, 0, 2, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 4, 5, 4, 0, 4, 8, 9, 8, 12, 13, 13, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  -61\n",
      "Actions taken during this episode:  [0, 2, 0, 1, 3, 2, 1, 0, 3, 2, 0, 0, 0, 2, 1, 3, 0, 3, 3, 1, 2, 2, 3, 3, 2, 3, 1, 1, 3, 0, 3, 3, 0, 2, 3, 0, 2, 2, 1, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 5, 4, 5, 1, 5, 4, 5, 9, 13, 13, 14, 10, 9, 13, 12, 12, 8, 9, 10, 9, 8, 9, 8, 4, 0, 0, 4, 4, 4, 8, 9, 8, 12, 13, 14, 10, 11, 15]\n",
      "Greedy actions taken 10 Random actions taken 32\n",
      "One episode complete\n",
      "Reward for this episode is:  -74\n",
      "Actions taken during this episode:  [2, 0, 1, 1, 0, 3, 2, 2, 3, 1, 1, 0, 2, 3, 1, 2, 2, 0, 2, 2, 0, 2, 2, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 1, 1, 5, 4, 5, 6, 5, 1, 1, 5, 6, 5, 1, 2, 3, 7, 7, 7, 11, 11, 11, 7, 11, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 23\n",
      "One episode complete\n",
      "Reward for this episode is:  -30\n",
      "Actions taken during this episode:  [1, 2, 3, 1, 2, 2, 1, 1, 3, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 0, 0, 1, 2, 2, 2, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  -19\n",
      "Actions taken during this episode:  [3, 0, 3, 1, 2, 0, 1, 1, 0, 0, 3, 0, 2, 2, 1, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 4, 0, 1, 5, 1, 1, 5, 9, 8, 12, 13, 14, 10, 11, 15]\n",
      "Greedy actions taken 3 Random actions taken 15\n",
      "One episode complete\n",
      "Reward for this episode is:  -27\n",
      "Actions taken during this episode:  [3, 2, 1, 1, 3, 2, 3, 2, 2, 0, 0, 1, 0, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 1, 1, 0, 1, 0, 1, 2, 6, 10, 6, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 11\n",
      "One episode complete\n",
      "Reward for this episode is:  -44\n",
      "Actions taken during this episode:  [3, 3, 1, 1, 0, 1, 1, 0, 2, 0, 2, 1, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 0, 0, 4, 0, 0, 4, 5, 9, 10, 6, 10, 11, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 13\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -2\n",
      "Actions taken during this episode:  [2, 1, 0, 0, 2, 0, 3, 1, 1, 0, 2, 1, 1, 0, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 1, 5, 9, 10, 14, 13, 9, 5, 9, 10, 6, 2, 6, 7, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 12\n",
      "One episode complete\n",
      "Reward for this episode is:  -14\n",
      "Actions taken during this episode:  [1, 1, 2, 1, 0, 3, 2, 0, 2, 1, 0, 0, 1, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 1, 5, 4, 5, 9, 10, 6, 10, 14, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 9\n",
      "One episode complete\n",
      "Reward for this episode is:  -4\n",
      "Actions taken during this episode:  [2, 2, 0, 3, 2, 1, 0, 0, 2, 1, 2, 3, 2, 1, 0, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 2, 6, 5, 6, 2, 6, 10, 11, 7, 7, 6, 7, 3, 7, 11, 15]\n",
      "Greedy actions taken 3 Random actions taken 15\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [2, 0, 0, 0, 0, 2, 1, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 13, 13, 14, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  -49\n",
      "Actions taken during this episode:  [0, 0, 2, 1, 1, 0, 0, 3, 3, 3, 3, 3, 2, 0, 0, 1, 1, 3, 2, 0, 3, 1, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 8, 9, 5, 1, 5, 9, 8, 8, 8, 8, 8, 9, 13, 13, 9, 5, 4, 5, 9, 8, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 10 Random actions taken 18\n",
      "One episode complete\n",
      "Reward for this episode is:  -9\n",
      "Actions taken during this episode:  [1, 2, 2, 3, 0, 2, 0, 2, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 1, 5, 6, 10, 11, 7, 11, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 11\n",
      "One episode complete\n",
      "Reward for this episode is:  -19\n",
      "Actions taken during this episode:  [2, 1, 3, 2, 0, 0, 2, 2, 3, 2, 3, 3, 2, 1, 1, 3, 1, 2, 0, 0, 0, 3, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 1, 0, 1, 5, 9, 10, 11, 10, 11, 10, 9, 10, 6, 2, 1, 1, 2, 6, 10, 14, 13, 14, 15]\n",
      "Greedy actions taken 8 Random actions taken 17\n",
      "One episode complete\n",
      "Reward for this episode is:  6\n",
      "Actions taken during this episode:  [2, 3, 3, 0, 2, 0, 2, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 0, 0, 4, 5, 9, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  -10\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 0, 0, 3, 1, 2, 0, 3, 2, 3, 2, 3, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 14, 14, 13, 9, 10, 14, 13, 14, 13, 14, 13, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 14\n",
      "One episode complete\n",
      "Reward for this episode is:  -12\n",
      "Actions taken during this episode:  [1, 2, 2, 0, 0, 0, 0, 1, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 6, 10, 14, 14, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 8\n",
      "One episode complete\n",
      "Reward for this episode is:  10\n",
      "Actions taken during this episode:  [2, 0, 0, 0, 1, 1, 3, 2, 0, 1, 0, 2, 3, 0, 3, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 13, 9, 5, 4, 5, 9, 5, 9, 10, 9, 13, 12, 13, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 13\n",
      "One episode complete\n",
      "Reward for this episode is:  8\n",
      "Actions taken during this episode:  [1, 2, 0, 2, 0, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 6, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  13\n",
      "Actions taken during this episode:  [0, 2, 0, 3, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 8, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [0, 2, 1, 0, 0, 2, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 1, 5, 9, 10, 11, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  -46\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 1, 0, 1, 3, 3, 2, 0, 1, 3, 3, 3, 2, 1, 3, 3, 2, 0, 0, 1, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 5, 9, 5, 4, 4, 5, 9, 5, 4, 4, 4, 5, 1, 0, 0, 1, 5, 9, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 10 Random actions taken 18\n",
      "One episode complete\n",
      "Reward for this episode is:  -22\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 1, 1, 1, 0, 0, 0, 3, 3, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 5, 1, 1, 5, 9, 13, 12, 12, 13, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 11\n",
      "One episode complete\n",
      "Reward for this episode is:  -22\n",
      "Actions taken during this episode:  [0, 1, 2, 3, 0, 1, 1, 3, 0, 2, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 0, 1, 0, 4, 0, 0, 0, 4, 5, 9, 13, 14, 14, 15]\n",
      "Greedy actions taken 3 Random actions taken 13\n",
      "One episode complete\n",
      "Reward for this episode is:  -1\n",
      "Actions taken during this episode:  [2, 1, 0, 0, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 1, 5, 9, 10, 11, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  -19\n",
      "Actions taken during this episode:  [3, 3, 2, 0, 3, 2, 2, 2, 1, 0, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 5, 4, 5, 6, 7, 3, 7, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [0, 0, 2, 2, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 8, 9, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  -56\n",
      "Actions taken during this episode:  [3, 2, 1, 1, 1, 2, 1, 2, 0, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 1, 1, 1, 2, 2, 3, 7, 11, 11, 15]\n",
      "Greedy actions taken 3 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  -3\n",
      "Actions taken during this episode:  [3, 2, 2, 3, 2, 0, 2, 3, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 1, 2, 6, 7, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 8\n",
      "One episode complete\n",
      "Reward for this episode is:  -19\n",
      "Actions taken during this episode:  [2, 3, 2, 3, 2, 2, 1, 0, 0, 2, 1, 3, 1, 1, 0, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 0, 1, 0, 1, 2, 2, 6, 10, 11, 7, 6, 2, 2, 6, 7, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 14\n",
      "One episode complete\n",
      "Reward for this episode is:  -1\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  18\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 7, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -6\n",
      "Actions taken during this episode:  [1, 2, 2, 2, 0, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 3, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [0, 2, 1, 0, 0, 2, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 1, 5, 9, 10, 6, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  15\n",
      "Actions taken during this episode:  [2, 0, 1, 0, 2, 3, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 1, 5, 6, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  -13\n",
      "Actions taken during this episode:  [1, 2, 3, 1, 3, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 0, 0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 3 Random actions taken 9\n",
      "One episode complete\n",
      "Reward for this episode is:  -18\n",
      "Actions taken during this episode:  [3, 2, 1, 2, 0, 3, 0, 0, 2, 3, 2, 3, 3, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 1, 2, 6, 5, 9, 13, 14, 13, 14, 13, 12, 13, 14, 15]\n",
      "Greedy actions taken 7 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  -1\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 1, 1, 0, 3, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 6, 2, 6, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 8 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  -36\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 3, 2, 2, 2, 1, 1, 2, 0, 0, 3, 3, 1, 3, 1, 2, 1, 3, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 9, 10, 11, 11, 7, 3, 3, 7, 11, 10, 9, 5, 4, 0, 1, 1, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 14 Random actions taken 15\n",
      "One episode complete\n",
      "Reward for this episode is:  12\n",
      "Actions taken during this episode:  [0, 2, 3, 2, 1, 0, 2, 0, 3, 2, 1, 0, 0, 3, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 4, 5, 1, 5, 6, 10, 9, 10, 6, 10, 14, 13, 14, 15]\n",
      "Greedy actions taken 10 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -15\n",
      "Actions taken during this episode:  [1, 2, 0, 3, 1, 0, 3, 0, 2, 0, 3, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 4, 0, 4, 4, 8, 9, 13, 12, 13, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 9\n",
      "One episode complete\n",
      "Reward for this episode is:  1\n",
      "Actions taken during this episode:  [0, 0, 3, 2, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 8, 8, 9, 10, 14, 15]\n",
      "Greedy actions taken 2 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  16\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 0, 1, 0, 3, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 14, 10, 14, 13, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  -46\n",
      "Actions taken during this episode:  [0, 1, 2, 1, 2, 3, 0, 2, 0, 2, 2, 2, 1, 3, 0, 1, 0, 1, 2, 2, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 4, 0, 1, 1, 2, 1, 5, 6, 10, 11, 11, 11, 7, 6, 10, 6, 10, 6, 7, 7, 7, 11, 15]\n",
      "Greedy actions taken 8 Random actions taken 16\n",
      "One episode complete\n",
      "Reward for this episode is:  13\n",
      "Actions taken during this episode:  [2, 0, 0, 3, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 8, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 2, 0, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  6\n",
      "Actions taken during this episode:  [3, 2, 0, 1, 0, 3, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 1, 5, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  -6\n",
      "Actions taken during this episode:  [1, 2, 2, 2, 0, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 3, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  4\n",
      "Actions taken during this episode:  [1, 2, 0, 2, 0, 2, 3, 2, 1, 0, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 6, 10, 11, 10, 11, 7, 11, 7, 11, 15]\n",
      "Greedy actions taken 9 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  8\n",
      "Actions taken during this episode:  [3, 2, 0, 3, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -10\n",
      "Actions taken during this episode:  [1, 3, 0, 0, 2, 2, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 4, 8, 9, 10, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [0, 1, 1, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  -6\n",
      "Actions taken during this episode:  [1, 2, 0, 2, 1, 0, 0, 1, 1, 0, 0, 1, 0, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 6, 2, 6, 10, 6, 2, 6, 10, 6, 10, 6, 10, 11, 15]\n",
      "Greedy actions taken 9 Random actions taken 9\n",
      "One episode complete\n",
      "Reward for this episode is:  3\n",
      "Actions taken during this episode:  [2, 1, 0, 2, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 1, 5, 6, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -7\n",
      "Actions taken during this episode:  [3, 1, 2, 0, 2, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 5, 6, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  1\n",
      "Actions taken during this episode:  [2, 3, 2, 0, 3, 2, 0, 1, 0, 2, 3, 2, 2, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 0, 1, 5, 4, 5, 9, 5, 9, 10, 9, 10, 11, 11, 7, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 11\n",
      "One episode complete\n",
      "Reward for this episode is:  -2\n",
      "Actions taken during this episode:  [3, 1, 2, 0, 0, 1, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 5, 9, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  -1\n",
      "Actions taken during this episode:  [3, 0, 0, 1, 0, 2, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 8, 4, 8, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  8\n",
      "Actions taken during this episode:  [2, 0, 0, 0, 0, 1, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 13, 13, 9, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  5\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 1, 1, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 5, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [0, 2, 0, 1, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -4\n",
      "Actions taken during this episode:  [0, 2, 3, 2, 2, 0, 3, 3, 1, 3, 2, 0, 2, 3, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 4, 5, 6, 10, 9, 8, 4, 4, 5, 9, 10, 9, 10, 11, 15]\n",
      "Greedy actions taken 8 Random actions taken 10\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  5\n",
      "Actions taken during this episode:  [3, 2, 0, 1, 0, 2, 0, 2, 3, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 1, 5, 6, 10, 11, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  18\n",
      "Actions taken during this episode:  [0, 1, 2, 0, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 0, 1, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  10\n",
      "Actions taken during this episode:  [2, 2, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 2, 6, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  -5\n",
      "Actions taken during this episode:  [3, 2, 0, 3, 2, 2, 2, 3, 3, 3, 2, 3, 2, 3, 2, 2, 0, 1, 0, 1, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 4, 5, 6, 7, 6, 5, 4, 5, 4, 5, 4, 5, 6, 10, 6, 10, 6, 10, 14, 15]\n",
      "Greedy actions taken 13 Random actions taken 11\n",
      "One episode complete\n",
      "Reward for this episode is:  5\n",
      "Actions taken during this episode:  [0, 2, 3, 3, 2, 3, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 4, 4, 5, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 2, 0, 3, 3, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 7, 11, 10, 9, 13, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [0, 2, 0, 1, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  11\n",
      "Actions taken during this episode:  [2, 0, 0, 1, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 5, 9, 13, 14, 15]\n",
      "Greedy actions taken 3 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  -4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Actions taken during this episode:  [3, 2, 0, 3, 3, 2, 1, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 4, 4, 5, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 7 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  8\n",
      "Actions taken during this episode:  [1, 0, 2, 0, 1, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 9, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  -3\n",
      "Actions taken during this episode:  [1, 0, 3, 2, 0, 2, 0, 1, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 4, 5, 9, 10, 14, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  1\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 3, 2, 2, 2, 3, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 8, 9, 10, 11, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  15\n",
      "Actions taken during this episode:  [0, 2, 3, 2, 0, 2, 0, 1, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 4, 5, 9, 10, 14, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  15\n",
      "Actions taken during this episode:  [2, 0, 3, 1, 0, 2, 0, 2, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 4, 0, 4, 5, 9, 10, 11, 7, 11, 15]\n",
      "Greedy actions taken 8 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  -1\n",
      "Actions taken during this episode:  [1, 3, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [2, 0, 1, 1, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 1, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [0, 3, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -6\n",
      "Actions taken during this episode:  [1, 2, 2, 0, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 6, 7, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  3\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 13, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  -12\n",
      "Actions taken during this episode:  [1, 3, 1, 2, 0, 0, 2, 2, 3, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 0, 0, 1, 5, 9, 10, 11, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 3 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  -8\n",
      "Actions taken during this episode:  [1, 2, 1, 0, 0, 3, 2, 2, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 1, 5, 9, 8, 9, 10, 6, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  10\n",
      "Actions taken during this episode:  [0, 0, 0, 2, 1, 2, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 4, 8, 12, 13, 9, 10, 11, 7, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 6\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  -2\n",
      "Actions taken during this episode:  [1, 1, 2, 0, 3, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 1, 5, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  0\n",
      "Actions taken during this episode:  [1, 2, 2, 0, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  0\n",
      "Actions taken during this episode:  [3, 2, 2, 0, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 2, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  3\n",
      "Actions taken during this episode:  [1, 2, 0, 1, 0, 0, 1, 0, 1, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 1, 5, 9, 5, 9, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 9 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [1, 2, 3, 2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  16\n",
      "Actions taken during this episode:  [2, 0, 3, 2, 0, 1, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 4, 5, 9, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  3\n",
      "Actions taken during this episode:  [1, 0, 2, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 9, 13, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  16\n",
      "Actions taken during this episode:  [0, 1, 2, 0, 3, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 0, 1, 5, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  18\n",
      "Actions taken during this episode:  [0, 2, 0, 0, 2, 1, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 13, 14, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 0, 2, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 9, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  8\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 7, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [3, 2, 0, 0, 2, 2, 3, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 9, 10, 11, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  12\n",
      "Actions taken during this episode:  [0, 1, 2, 0, 0, 0, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 0, 1, 5, 9, 13, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -3\n",
      "Actions taken during this episode:  [3, 2, 0, 1, 1, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 1, 5, 1, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  13\n",
      "Actions taken during this episode:  [0, 2, 2, 2, 0, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 7, 11, 15]\n",
      "Greedy actions taken 2 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 3, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 9, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  10\n",
      "Actions taken during this episode:  [2, 0, 0, 2, 1, 3, 1, 3, 2, 3, 2, 0, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 9, 10, 6, 5, 1, 0, 1, 0, 1, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 10 Random actions taken 7\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 0, 2, 0, 2, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 9, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 1, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 6, 10, 14, 15]\n",
      "Greedy actions taken 7 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  11\n",
      "Actions taken during this episode:  [2, 0, 2, 1, 0, 0, 0, 3, 2, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 2, 6, 10, 14, 13, 14, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 0, 1, 0, 2]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 14, 10, 14, 15]\n",
      "Greedy actions taken 7 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  5\n",
      "Actions taken during this episode:  [0, 0, 0, 2, 2, 2]\n",
      "States traversed during this episode:  [0, 4, 8, 12, 13, 14, 15]\n",
      "Greedy actions taken 3 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [0, 3, 2, 3, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 4, 5, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [2, 1, 0, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 1, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  18\n",
      "Actions taken during this episode:  [0, 2, 1, 0, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 1, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  15\n",
      "Actions taken during this episode:  [0, 2, 2, 3, 2, 0, 1, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 5, 6, 10, 6, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 2, 3, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [2, 0, 2, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 10, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 0, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 14, 14, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 0, 1, 0, 2]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 14, 10, 14, 15]\n",
      "Greedy actions taken 6 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 4 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  15\n",
      "Actions taken during this episode:  [0, 2, 2, 3, 2, 3, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 5, 6, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 7 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  7\n",
      "Actions taken during this episode:  [3, 0, 2, 3, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [2, 0, 2, 0, 2, 3, 2, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 10, 11, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  -8\n",
      "Actions taken during this episode:  [1, 3, 0, 2, 2, 1, 0, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 0, 4, 5, 6, 2, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 5\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [1, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  9\n",
      "Actions taken during this episode:  [3, 0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  18\n",
      "Actions taken during this episode:  [2, 0, 2, 0, 2, 1, 0, 0]\n",
      "States traversed during this episode:  [0, 1, 5, 6, 10, 11, 7, 11, 15]\n",
      "Greedy actions taken 6 Random actions taken 3\n",
      "One episode complete\n",
      "Reward for this episode is:  -1\n",
      "Actions taken during this episode:  [1, 0, 3, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 0, 4, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "One episode complete\n",
      "Reward for this episode is:  17\n",
      "Actions taken during this episode:  [0, 2, 2, 3, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 4\n",
      "One episode complete\n",
      "Reward for this episode is:  19\n",
      "Actions taken during this episode:  [0, 2, 2, 0, 2, 0]\n",
      "States traversed during this episode:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Greedy actions taken 5 Random actions taken 2\n",
      "-----------------------------------------------------------------\n",
      "Maximum reward in all episodes:  19\n",
      "Winning State Actions:  [0, 4, 5, 6, 10, 11, 15]\n",
      "Final Q Table [[ 5.72868302 -6.28647271  5.08501202 -6.31837128]\n",
      " [ 7.87947083 -4.66910661 -1.81799908  2.48198242]\n",
      " [ 6.17141011 -6.46633815  2.51517175  2.96901597]\n",
      " [ 4.07416061 -8.10066312 -7.80240075  0.81850363]\n",
      " [-0.54321897  2.49656229  8.17678883 -4.75988567]\n",
      " [ 5.71590427  4.8844533   7.36751297  4.39207634]\n",
      " [10.88823883 -1.02592318  4.40187203  4.15575172]\n",
      " [ 6.32238003  2.40971131 -5.61653508  5.88434555]\n",
      " [ 2.50388897  2.18641328  6.03056778 -7.15486495]\n",
      " [ 4.78814855  4.05947812  8.5713483  -1.22751061]\n",
      " [ 6.54814968  6.36865603  6.99754058  6.13330242]\n",
      " [10.          4.53373395 -3.00034183  4.10763913]\n",
      " [-8.24826025 -0.33243295  4.25998715 -7.86662843]\n",
      " [-5.4584615   5.67336632  6.74838946  2.51122501]\n",
      " [-3.00060824  4.9866655  10.          4.47152464]\n",
      " [ 0.          0.          0.          0.        ]]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "display2() takes 2 positional arguments but 3 were given",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-551-3a18113d9a08>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mttm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mTrainAndTestModels\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mttm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnavigate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'QL'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'D'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-550-3dd67d4df19d>\u001b[0m in \u001b[0;36mnavigate\u001b[0;34m(self, function_type, env_type)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridEnvironment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'QL'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0mqtable_Q_learning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQTableQL\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32melif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'SARSA'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mqtable_Q_learning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetQTableSARSA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-550-3dd67d4df19d>\u001b[0m in \u001b[0;36mgetQTableQL\u001b[0;34m(self, env_type, env)\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisilon_decay_array\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 89\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdisplay2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwinning_state\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards_in_episode_for_optimal_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mqtable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: display2() takes 2 positional arguments but 3 were given"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "ttm=TrainAndTestModels()\n",
    "ttm.navigate('QL','D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

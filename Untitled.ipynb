{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb90c3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReinforcementLearning():\n",
    "    def __init__(self):\n",
    "        print(\"Welcome......\")\n",
    "        env = GridEnvironment()\n",
    "        num_of_episodes=2\n",
    "        reward_per_episode_array=[]\n",
    "        qtables_array=[]\n",
    "        winning_state=[]\n",
    "        episilon_decay_array=[]\n",
    "\n",
    "    def navigate(self,function_type,env_type):\n",
    "        if(function_type=='QL'):\n",
    "            qtable_Q_learning=self.getQTableQL(env_type)\n",
    "        elif(function_type=='SARSA'):\n",
    "            qtable_Q_learning=self.getQTableSARSA(env_type)\n",
    "\n",
    "    def getQTableQL(self,env_type):\n",
    "        print(\"<-------------------Q-Learning-------------------->\")\n",
    "        if(env_type=='D'):\n",
    "            print(\"<-------------------Deterministic Environment-------------------->\")\n",
    "        elif(env_type=='S'):\n",
    "            print(\"<-------------------Stochastic Environment-------------------->\")\n",
    "        for episode in range(num_of_episodes):\n",
    "            #learning rate\n",
    "            alpha=0.4\n",
    "            #discount rate\n",
    "            gamma=0.7\n",
    "    \n",
    "            exploration_rate = 1\n",
    "            max_exploration_rate = 1\n",
    "            min_exploration_rate = 0.01\n",
    "            exploration_decay_rate = 0.01\n",
    "        \n",
    "            env.reset()\n",
    "            cum_reward_per_episode=0\n",
    "            actions_in_one_episode=[]\n",
    "            states_passed_in_one_episode=[]\n",
    "            states_passed_in_one_episode.append(env.mapping[tuple(env.agent_pos)])\n",
    "            greedy_a=0\n",
    "            random_a=1\n",
    "            while True:\n",
    "#             print(\"EPISODE: \",episode)\n",
    "                state=env.mapping[tuple(env.agent_pos)]\n",
    "                qtable_state=env.qtable[state]\n",
    "                greedy_action=np.argmax(qtable_state)\n",
    "                greedy_q_value= max(qtable_state)\n",
    "        \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    action=greedy_action\n",
    "                    greedy_a=greedy_a+1\n",
    "                else:\n",
    "                    action=env.action_space.sample()\n",
    "                    random_a=random_a+1\n",
    "                episilon_decay_array.append(exploration_rate)\n",
    "\n",
    "                new_observation, reward, done, _ = env.step('D',action)\n",
    "                        \n",
    "                if(new_observation in states_passed_in_one_episode):\n",
    "                    reward= -1            \n",
    "                if(new_observation==state):\n",
    "                    reward= -10\n",
    "            \n",
    "                qtable_state[action]=qtable_state[action]+(alpha*(reward+gamma*max(env.qtable[new_observation])-qtable_state[[action]]))\n",
    "            \n",
    "                env.qtable[state]=qtable_state\n",
    "            \n",
    "                actions_in_one_episode.append(action)\n",
    "                states_passed_in_one_episode.append(new_observation)\n",
    "                cum_reward_per_episode=cum_reward_per_episode+reward\n",
    "            \n",
    "                exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "            \n",
    "                if done==True:\n",
    "#                     print(\"One episode complete\")\n",
    "#                     print(\"Reward for this episode is: \",cum_reward_per_episode)\n",
    "#                     print(\"Actions taken during this episode: \",actions_in_one_episode)\n",
    "#                     print(\"States traversed during this episode: \",states_passed_in_one_episode)\n",
    "#                     print(\"Greedy actions taken\",greedy_a,\"Random actions taken\",random_a)\n",
    "                    reward_per_episode_array.append(cum_reward_per_episode)\n",
    "                    qtables_array.append(env.qtable)\n",
    "                    if(cum_reward_per_episode >= max(reward_per_episode_array)):\n",
    "                        winning_state=states_passed_in_one_episode\n",
    "                        break\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#         print(\"Maximum reward in all episodes: \",max(reward_per_episode_array))\n",
    "#         print(\"Winning State Actions: \",winning_state)\n",
    "#         print(env.qtable)\n",
    "        plt.plot(reward_per_episode_array)\n",
    "        plt.figure() \n",
    "        plt.plot(episilon_decay_array)\n",
    "        self.display(winning_state)\n",
    "        return env.qtable\n",
    "\n",
    "\n",
    "    def getQTableSARSA(self,env_type):\n",
    "        print(\"<-------------------SARSA-------------------->\")\n",
    "        if(env_type=='D'):\n",
    "            print(\"<-------------------Deterministic Environment-------------------->\")\n",
    "        elif(env_type=='S'):\n",
    "            print(\"<-------------------Stochastic Environment-------------------->\")\n",
    "        \n",
    "        for episode in range(num_of_episodes):\n",
    "            #learning rate\n",
    "            alpha=0.4\n",
    "            #discount rate\n",
    "            gamma=0.7\n",
    "    \n",
    "            exploration_rate = 1\n",
    "            max_exploration_rate = 1\n",
    "            min_exploration_rate = 0.01\n",
    "            exploration_decay_rate = 0.01\n",
    "        \n",
    "            env.reset()\n",
    "            cum_reward_per_episode=0\n",
    "            actions_in_one_episode=[]\n",
    "            states_passed_in_one_episode=[]\n",
    "            states_passed_in_one_episode.append(env.mapping[tuple(env.agent_pos)])\n",
    "            greedy_a=0\n",
    "            random_a=1\n",
    "            while True:\n",
    "#             print(\"EPISODE: \",episode)\n",
    "                state=env.mapping[tuple(env.agent_pos)]\n",
    "                qtable_state=env.qtable[state]\n",
    "                greedy_action=np.argmax(qtable_state)\n",
    "                greedy_q_value= max(qtable_state)\n",
    "        \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    action=greedy_action\n",
    "                    greedy_a=greedy_a+1\n",
    "                else:\n",
    "                    action=env.action_space.sample()\n",
    "                    random_a=random_a+1\n",
    "                \n",
    "                episilon_decay_array.append(exploration_rate)\n",
    "            \n",
    "                new_observation, reward, done, _ = env.step('D',action)\n",
    "                        \n",
    "                if(new_observation in states_passed_in_one_episode):\n",
    "                    reward= -1            \n",
    "                if(new_observation==state):\n",
    "                    reward= -10\n",
    "            \n",
    "            #Calculating a' for s'\n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    new_action=greedy_action\n",
    "                else:\n",
    "                    new_action=env.action_space.sample()\n",
    "                \n",
    "                qtable_state[action]=qtable_state[action]+(alpha*(reward+gamma*(env.qtable[new_observation][new_action])-qtable_state[[action]]))\n",
    "            \n",
    "                env.qtable[state]=qtable_state\n",
    "            \n",
    "                actions_in_one_episode.append(action)\n",
    "                states_passed_in_one_episode.append(new_observation)\n",
    "                cum_reward_per_episode=cum_reward_per_episode+reward\n",
    "            \n",
    "                exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "            \n",
    "                if done==True:\n",
    "#                     print(\"One episode complete\")\n",
    "#                     print(\"Reward for this episode is: \",cum_reward_per_episode)\n",
    "#                     print(\"Actions taken during this episode: \",actions_in_one_episode)\n",
    "#                     print(\"States traversed during this episode: \",states_passed_in_one_episode)\n",
    "#                     print(\"Greedy actions taken\",greedy_a,\"Random actions taken\",random_a)\n",
    "                    reward_per_episode_array.append(cum_reward_per_episode)\n",
    "                    qtables_array.append(env.qtable)\n",
    "                    if(cum_reward_per_episode >= max(reward_per_episode_array)):\n",
    "                        winning_state=states_passed_in_one_episode\n",
    "                        break\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "#         print(\"Maximum reward in all episodes: \",max(reward_per_episode_array))\n",
    "#         print(\"Winning State Traversal: \",winning_state)\n",
    "#         print(env.qtable)\n",
    "        plt.plot(reward_per_episode_array)\n",
    "        plt.figure() \n",
    "        plt.plot(episilon_decay_array)\n",
    "        self.display(winning_state)\n",
    "        return env.qtable\n",
    "\n",
    "    def display(self,winning_states):\n",
    "        env.reset()\n",
    "        inv_map = {v: k for k, v in env.mapping.items()}\n",
    "        for i in range(len(winning_states)):\n",
    "            clear_output(wait=True)\n",
    "            env.agent_pos=list(inv_map.get(winning_states[i]))\n",
    "            env.render2()\n",
    "            sleep(.1)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e18767e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainAndTestModels():\n",
    "    def __init__(self):\n",
    "        print(\"Training and Testing the Models\")\n",
    "        self.env = GridEnvironment()\n",
    "        \n",
    "    def navigate(self,function_type,env_type):\n",
    "        if(function_type=='QL'):\n",
    "            qtable_Q_learning=getQTableQL(env_type)\n",
    "        elif(function_type=='SARSA'):\n",
    "            qtable_Q_learning=getQTableSARSA(env_type,self.env)\n",
    "\n",
    "    def getQTableQL(env_type):\n",
    "        print(\"<-------------------Q-Learning-------------------->\")\n",
    "        if(env_type=='D'):\n",
    "            print(\"<-------------------Deterministic Environment-------------------->\")\n",
    "        elif(env_type=='S'):\n",
    "            print(\"<-------------------Stochastic Environment-------------------->\")\n",
    "        for episode in range(self.env.num_of_episodes):\n",
    "            #learning rate\n",
    "            alpha=0.4\n",
    "            #discount rate\n",
    "            gamma=0.7\n",
    "    \n",
    "            exploration_rate = 1\n",
    "            max_exploration_rate = 1\n",
    "            min_exploration_rate = 0.01\n",
    "            exploration_decay_rate = 0.01\n",
    "        \n",
    "            self.env.reset()\n",
    "            cum_reward_per_episode=0\n",
    "            actions_in_one_episode=[]\n",
    "            states_passed_in_one_episode=[]\n",
    "            rewards_in_episode=[]\n",
    "            states_passed_in_one_episode.append(self.env.mapping[tuple(self.env.agent_pos)])\n",
    "            greedy_a=0\n",
    "            random_a=1\n",
    "            while True:\n",
    "                state=self.env.mapping[tuple(self.env.agent_pos)]\n",
    "                qtable_state=self.env.qtable[state]\n",
    "                greedy_action=np.argmax(qtable_state)\n",
    "                greedy_q_value= max(qtable_state)\n",
    "        \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    action=greedy_action\n",
    "                    greedy_a=greedy_a+1\n",
    "                else:\n",
    "                    action=self.env.action_space.sample()\n",
    "                    random_a=random_a+1\n",
    "                self.env.episilon_decay_array.append(exploration_rate)\n",
    "\n",
    "                new_observation, reward, done, _ = self.env.step('D',action)\n",
    "                        \n",
    "                if(new_observation in states_passed_in_one_episode):\n",
    "                    reward= -1            \n",
    "                if(new_observation==state):\n",
    "                    reward= -10\n",
    "            \n",
    "                qtable_state[action]=qtable_state[action]+(alpha*(reward+gamma*max(self.env.qtable[new_observation])-qtable_state[[action]]))\n",
    "            \n",
    "                self.env.qtable[state]=qtable_state\n",
    "            \n",
    "                actions_in_one_episode.append(action)\n",
    "                states_passed_in_one_episode.append(new_observation)\n",
    "                cum_reward_per_episode=cum_reward_per_episode+reward\n",
    "                rewards_in_episode.append(reward)\n",
    "\n",
    "                exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "            \n",
    "                if done==True:\n",
    "                    print(\"One episode complete\")\n",
    "                    print(\"Reward for this episode is: \",cum_reward_per_episode)\n",
    "                    print(\"Actions taken during this episode: \",actions_in_one_episode)\n",
    "                    print(\"States traversed during this episode: \",states_passed_in_one_episode)\n",
    "                    print(\"Greedy actions taken\",greedy_a,\"Random actions taken\",random_a)\n",
    "                    self.env.reward_per_episode_array.append(cum_reward_per_episode)\n",
    "                    self.env.qtables_array.append(self.env.qtable)\n",
    "                    if(cum_reward_per_episode >= max(env.reward_per_episode_array)):\n",
    "                        self.env.winning_state=states_passed_in_one_episode\n",
    "                        self.env.rewards_in_episode_for_optimal_policy=rewards_in_episode\n",
    "                    break\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Maximum reward in all episodes: \",max(self.env.reward_per_episode_array))\n",
    "        print(\"Winning State Actions: \",self.env.winning_state)\n",
    "        print(\"Final Q Table\",self.env.qtable)\n",
    "        plt.plot(self.env.reward_per_episode_array)\n",
    "        plt.figure() \n",
    "        plt.plot(self.env.episilon_decay_array)\n",
    "#         self.display(env.winning_state,env.rewards_in_episode_for_optimal_policy)\n",
    "        self.display2(self.env.qtable)\n",
    "        return self.env.qtable\n",
    "\n",
    "\n",
    "    def getQTableSARSA(self,env_type,env):\n",
    "        print(\"<-------------------SARSA-------------------->\")\n",
    "        if(env_type=='D'):\n",
    "            print(\"<-------------------Deterministic Environment-------------------->\")\n",
    "        elif(env_type=='S'):\n",
    "            print(\"<-------------------Stochastic Environment-------------------->\")\n",
    "        \n",
    "        for episode in range(env.num_of_episodes):\n",
    "            #learning rate\n",
    "            alpha=0.4\n",
    "            #discount rate\n",
    "            gamma=0.7\n",
    "    \n",
    "            exploration_rate = 1\n",
    "            max_exploration_rate = 1\n",
    "            min_exploration_rate = 0.01\n",
    "            exploration_decay_rate = 0.01\n",
    "        \n",
    "            env.reset()\n",
    "            cum_reward_per_episode=0\n",
    "            actions_in_one_episode=[]\n",
    "            states_passed_in_one_episode=[]\n",
    "            rewards_in_episode=[]\n",
    "            states_passed_in_one_episode.append(env.mapping[tuple(env.agent_pos)])\n",
    "            greedy_a=0\n",
    "            random_a=1\n",
    "            while True:\n",
    "                state=env.mapping[tuple(env.agent_pos)]\n",
    "                qtable_state=env.qtable[state]\n",
    "                greedy_action=np.argmax(qtable_state)\n",
    "                greedy_q_value= max(qtable_state)\n",
    "        \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    action=greedy_action\n",
    "                    greedy_a=greedy_a+1\n",
    "                else:\n",
    "                    action=env.action_space.sample()\n",
    "                    random_a=random_a+1\n",
    "                \n",
    "                env.episilon_decay_array.append(exploration_rate)\n",
    "            \n",
    "                new_observation, reward, done, _ = env.step('D',action)\n",
    "                        \n",
    "                if(new_observation in states_passed_in_one_episode):\n",
    "                    reward= -1            \n",
    "                if(new_observation==state):\n",
    "                    reward= -10\n",
    "           \n",
    "                exploration_rate = min_exploration_rate + (max_exploration_rate - min_exploration_rate) * np.exp(-exploration_decay_rate*episode)\n",
    "           \n",
    "                expl_rate_threshold=random.uniform(0,1)\n",
    "                if expl_rate_threshold> exploration_rate:\n",
    "                    new_action=greedy_action\n",
    "                else:\n",
    "                    new_action=env.action_space.sample()\n",
    "                \n",
    "                qtable_state[action]=qtable_state[action]+(alpha*(reward+gamma*(env.qtable[new_observation][new_action])-qtable_state[[action]]))\n",
    "            \n",
    "                env.qtable[state]=qtable_state\n",
    "            \n",
    "                actions_in_one_episode.append(action)\n",
    "                states_passed_in_one_episode.append(new_observation)\n",
    "                cum_reward_per_episode=cum_reward_per_episode+reward\n",
    "                rewards_in_episode.append(reward)\n",
    "                        \n",
    "                if done==True:\n",
    "                    print(\"One episode complete\")\n",
    "                    print(\"Reward for this episode is: \",cum_reward_per_episode)\n",
    "                    print(\"Actions taken during this episode: \",actions_in_one_episode)\n",
    "                    print(\"States traversed during this episode: \",states_passed_in_one_episode)\n",
    "                    print(\"Greedy actions taken\",greedy_a,\"Random actions taken\",random_a)\n",
    "                    env.reward_per_episode_array.append(cum_reward_per_episode)\n",
    "                    env.qtables_array.append(env.qtable)\n",
    "                    if(cum_reward_per_episode >= max(self.env.reward_per_episode_array)):\n",
    "                        env.winning_state=states_passed_in_one_episode\n",
    "                        env.rewards_in_episode_for_optimal_policy=rewards_in_episode\n",
    "                    break\n",
    "        print(\"-----------------------------------------------------------------\")\n",
    "        print(\"Maximum reward in all episodes: \",max(env.reward_per_episode_array))\n",
    "        print(\"Winning State Traversal: \",env.winning_state)\n",
    "        print(\"FINAL Q TABLE: \",env.qtable)\n",
    "        plt.plot(env.reward_per_episode_array)\n",
    "        plt.figure() \n",
    "        plt.plot(env.episilon_decay_array)\n",
    "#     display(env.winning_state,env.rewards_in_episode_for_optimal_policy)\n",
    "        self.display2(env.qtable)\n",
    "        return env.qtable\n",
    "\n",
    "    def display(self,winning_states,winning_state_rewards):\n",
    "        print(\"Winning state rewards\",winning_state_rewards)\n",
    "        inv_map = {v: k for k, v in env.mapping.items()}\n",
    "        for i in range(len(winning_states)):\n",
    "            print(list(inv_map.get(winning_states[i])))\n",
    "            env.agent_pos=list(inv_map.get(winning_states[i]))\n",
    "            env.render2()\n",
    "            sleep(.1)\n",
    "\n",
    "    def display2(self,qtable):\n",
    "        env.reset()\n",
    "        print(\"Using Q table to move though the optimal states......\")\n",
    "        state=env.mapping[(0,0)]\n",
    "        inv_map = {v: k for k, v in env.mapping.items()}\n",
    "        while True:\n",
    "            qtable_state=env.qtable[state]\n",
    "            optimal_action=np.argmax(qtable_state)\n",
    "            new_observation, reward, done, _ = env.step('D',optimal_action)\n",
    "            state=new_observation\n",
    "            print(list(inv_map.get(state)))\n",
    "            env.agent_pos=list(inv_map.get(state))\n",
    "            env.render2()\n",
    "            sleep(.1)\n",
    "            if(done):\n",
    "                break\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
